{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Phenomena\n",
    "\n",
    "Quantum effects can be very counterintuitive. <font color='red'>In some sense</font>, our intuition is not built for a quantum world because the macroscopic world looks very classical. An excellent example of quantum magic is quantum tunneling.\n",
    "\n",
    "Imagine a particle travelling along a potential energy surface $V(x)$ (time independent) with some kinetic energy ($T$). The total energy ($E$) is simply:\n",
    "\n",
    "$$ E = T + V $$\n",
    "\n",
    "If there is a potential energy barrier which is higher than the total energy $E$ of our particle, then classically the particle will never get past the barrier. <font color='red'>Physically, that makes sense - the only way to throw a ball over a hill is to put enough energy to throw it over. </font>\n",
    "\n",
    "However, in the quantum regime, we have to consider the wave-like nature of our particle. As you may remember, when a wave encounters a barrier, some of it will be reflected and some of it will be transmitted <font color='red'>through</font>. This suggests that, as long as the energy barrier is finitely high, the particle may in fact slip through.\n",
    "\n",
    "This fact, combined with de Broglie's realization that all matter is a wave, brings us to a startling realization. Say we have a glass of water (ignoring effects like evaporation). If we wait long enough, the water molecules will tunnel through the glass (instead of jumping over it). Don't try this at home though, chances are you'll see the heat death of the universe long before any molecules get through.\n",
    "\n",
    "<font color='red'>We will discuss this in more detail in Chapter 10.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually observe some tunneling by numerically solving Schrödinger's equation, just like in the `schroedinger_equation.ipynb` notebook, simply by choosing the potential energy function $V(x)$ carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300\n",
    "L = 100\n",
    "w = 30\n",
    "\n",
    "def V(x):\n",
    "    n = len(x)\n",
    "    V = np.ones(n)*0\n",
    "    V[np.abs(x)<=w/2] = 0.05\n",
    "    return V\n",
    "\n",
    "eta = 1\n",
    "m = 1\n",
    "q = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0,L,num=N) - L/2\n",
    "a = X[1] - X[0] # grid spacing\n",
    "\n",
    "t = -eta**2 / (2 * m * a**2)\n",
    "eps = -2*t + q * V(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "plt.plot(X, V(X))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('V(x)')\n",
    "plt.title('Potential Energy Surface')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we the hamiltonian for our discretized system, and solve for the eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = t*np.eye(N, k=-1) + eps*np.eye(N) + t*np.eye(N, k=1) # discretized hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, vecs = np.linalg.eig(H)\n",
    "# vals.shape, vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(vals)\n",
    "vals, vecs = vals[order], vecs[:, order]\n",
    "vecs = vecs.T\n",
    "vecs /= np.sqrt(a)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We are going to plot the wavefunction of the lowest energy eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential Energy Figure\n",
    "plt.figure(figsize=(8,2))\n",
    "plt.plot(X, V(X))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('V(x)')\n",
    "plt.title('Potential Energy Surface')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Lowest Energy Eigenfunction\n",
    "plt.figure(figsize=(8,2))\n",
    "plt.plot(X, vecs[0,:])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Psi 0')\n",
    "plt.title('Lowest Energy Eigenfunction of a Particle in a Box With a Barrier (E_0 = {:.4f})'.format(vals[0]))\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"\\nThe lowest energy eigenvalue (E_0 = {vals[0]:.4f}) is smaller than the potential energy from x = -15 to x = 15, \" +\n",
    "        f\"which is 0.05 (see figure above). Yet, the energy eigenfunction is non-zero in this range. Classically, it would \" +\n",
    "        f\"be impossible to find a particle with a total energy of {vals[0]:.4f} in the range from x = -15 to 15.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest energy eigenvalue (E_0 = 0.0033) is smaller than the potential energy from x = -15 to x = 15, which is 0.05 (see figure above). Yet, the energy eigenfunction is non-zero in this range. Classically, it would be impossible to find a particle with a total energy of 0.0033 in the range from x = -15 to 15.\n",
    "\n",
    "Note - If you change the potential energy function, the lowest energy eigenvalue will be different. See print statement above for the correct energy eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to plot the energy eigenfunctions and probability density of the lowest 8 eigenfunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "fig, axes = plt.subplots(8, sharex=True, sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plt.title('Energy eigenfunctions of a particle in a box, with a barrier')\n",
    "for i, (ax, psi, E) in enumerate(zip(axes, vecs, vals)):\n",
    "    plt.sca(ax)\n",
    "    plt.ylabel('Psi {}'.format(i))\n",
    "    plt.plot(X, psi)\n",
    "    #plt.fill_between(X, psi, alpha=0.3)\n",
    "    print('E_{} = {:.4f}'.format(i, E))\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the computed energies for the first 8 levels and compare them to the height of our potential energy barrier. Remember that the eigenvalues of our hamiltonian are the total energies $E$ of each state.\n",
    "\n",
    "Just as before, we'll use our wavefunctions to predict the outcomes of measurements of the positions of our particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.abs(vecs)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, sharex=True, sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plt.title('Position Distributions of a Particle in a Box with Barrier')\n",
    "for i, (ax, pᵢ) in enumerate(zip(axes, p)):\n",
    "    plt.sca(ax)\n",
    "    plt.ylabel('p_{}(x)'.format(i))\n",
    "    plt.plot(X, pᵢ)\n",
    "    plt.fill_between(X, pᵢ, alpha=0.3)\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot above, what do you notice about the distributions of the even and odd solutions we found?\n",
    "\n",
    "For a particle in a box (considered in `schroedinger equation.ipynb`) we saw that our solutions for the wavefunction alternated between being even and odd (remember what it means for a function to be even/odd?). Now our potential is very similar, except that there is a barrier in the middle, resulting in a well on either side of the barrier. If we look at our wavefunctions for only a single well, they resemble the solutions to the particle in the box.\n",
    "\n",
    "If we think of our system as being two particles in a box next to each other, then what differentiates the ground state from the first excited state in our system? Looking at the plotted wavefunctions, we see in the ground state our two \"imaginary\" particles are in phase, while in the first excited state the particles are out of phase. In fact, this is related to electrons of different atoms interacting to form bonding and antibonding orbitals in molecules (for further reading). For now, just note the similarities between this system and the particle in the box, and understand the resulting trends in the energy spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the probability of measuring our particle in the barrier for each energy level using our probability density functions.\n",
    "To find the probability of some event $B$, we integrate our probability density function over all states in $B$. Since we discretized our probability density function, the integral turns into a sum using our grid spacing $a$.\n",
    "\n",
    "$$ \\mathrm{Prob}(B) = \\int_{x \\in B} p(x) dx = a \\sum_{x \\in B} p(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = V(X) != 0\n",
    "\n",
    "barrier = a * p[:, B].sum(-1)\n",
    "\n",
    "num = 8\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "\n",
    "plt.sca(ax1)\n",
    "plt.title('Energy Eigenvalue')\n",
    "plt.plot(np.arange(num), vals[:num], color='b', ls='', marker='x')\n",
    "plt.plot(np.arange(num), np.ones(num)*V(X)[B].mean(), ls='--')\n",
    "plt.xlabel('Level')\n",
    "plt.ylabel('Energy (arb units)')\n",
    "#ax1.tick_params('y', colors='b')\n",
    "\n",
    "#ax2 = ax1.twinx()\n",
    "plt.sca(ax2)\n",
    "plt.title('Probability of Being in Barrier')\n",
    "plt.plot(np.arange(num), barrier[:num], color='r', ls='', marker='o')\n",
    "plt.xlabel('Level')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "print()\n",
    "print('Level | Energy Eigenvalue | Prob(B)')\n",
    "print('------+-------------------+--------')\n",
    "for i, (E, prob_barrier) in enumerate(zip(vals[:8], barrier)):\n",
    "    print('{:6}|       {:.4f}      |  {:.4f}'.format(i, E, prob_barrier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reflect on the results a little bit. Specifically, look at the fourth excited state (level 4), note that the total energy of this state is significantly lower (about $0.03$) than our barrier ($0.05$). Yet, the probability of finding the particle in the barrier is over 7%, which means we only have to repeat this experiment around 14 times before we expect to find the particle in the barrier. \n",
    "\n",
    "If we know the particle is in the fourth excited state, and we find it inside the barrier, then we know the total energy $E \\approx 0.03$ and the potential energy $V = 0.05$. This would suggest the kinetic energy is negative. Recall the definition of the kinetic energy $T = \\frac{p^2}{2m}$, the mass is certainly not negative, and we can't make sense of an imaginary momentum - so how can we make sense of this?\n",
    "\n",
    "We can conclude that in a quantum world, we simply can't know the kinetic energy when we've measured the position. In other words, these two measurements don't commute. This is a very conceptual derivation of the uncertainty principle, which is usually first introduced between position and momentum, but as kinetic energy is related to momentum, the same principle holds for position and kinetic energy. \n",
    "\n",
    "More formally, given two operators $\\hat{A}$ and $\\hat{B}$, their commutation relation is given by operator $ \\left[\\hat{A}, \\hat{B} \\right] $:\n",
    "\n",
    "$$ \\left[\\hat{A}, \\hat{B} \\right] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A} $$\n",
    "\n",
    "$\\hat{A}$ and $\\hat{B}$ are said to commute if and only if their commutation relation is zero, $\\left[\\hat{A}, \\hat{B} \\right] = 0$.\n",
    "\n",
    "This is slightly sloppy notation, as operators without functions to act on do not make well formed expressions, so strictly speaking, we should say $\\hat{A}$ and $\\hat{B}$ commute if and only if $\\left[\\hat{A}, \\hat{B} \\right]=0$ for any $\\phi$\n",
    "\n",
    "$$ (\\hat{A}\\hat{B} - \\hat{B}\\hat{A}) \\phi = \\left[\\hat{A}, \\hat{B} \\right] \\phi = 0$$\n",
    "\n",
    "Note - the hat (^) is used to remind you that A and B are operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Position vs. Kinetic Energy Commutation\n",
    "\n",
    "<font color='purple'>This is not being assigned as a homework problem.</font>\n",
    "\n",
    "Show that the position operator and the kinetic energy operator do not commute and derive their commutation relation, given their definitions below.\n",
    "\n",
    "$$ \\hat{x} = x$$\n",
    "$$ \\hat{T} = -\\frac{\\hbar^2}{2m} \\frac{\\partial^2}{\\partial x^2} = \\frac{\\hat{p}^2}{2m} $$\n",
    "\n",
    "$$ [\\hat{x}, \\hat{T}] =~?$$\n",
    "\n",
    "Hint: a possibly useful property of commutation relations: $ \\left[\\hat{A}, \\hat{B}^2 \\right] = \\left[\\hat{A}, \\hat{B} \\right] \\hat{B} + \\hat{B} \\left[\\hat{A}, \\hat{B} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superposition\n",
    "\n",
    "<font color='purple'>The symbol alpha ($\\alpha_m$) is used for eigenvalues and a ($a_m$) is used for coefficients in superposition.</font>\n",
    "\n",
    "Classically the state of a system can be defined in terms of observables, such as position and momentum. Incidentally, for any physical system, if you specify the positions and momenta of all degrees of freedom then you have fully specified the state. \n",
    "\n",
    "However, in quantum mechanics we treat the state of our system (the wavefunction) separately from the observables (which correspond to operators). With this formalism, the outcome of a measurement (applying an operator to the wavefunction) corresponds to an eigenvalue of the operator corresponding to that observable. Since the measurements we make must be real, it follows that all eigenvalues of operators for observables are real, so the operators must be Hermitian.\n",
    "\n",
    "So far so good, if our state is in an eigenstate of some observable, then when we make the corresponding measurement, we'll get the corresponding eigenvalue as the outcome. However, the Hamiltonian is a linear operator, so any linear combination of solutions to the Schrödinger equation is also a valid solution. So how do we interpret linear combinations, aka a superposition, of eigenstates?\n",
    "\n",
    "Given some operator $\\mathbf{\\hat{O}}$ with eigenvalues $\\alpha_m$ and corresponding eigenstates (or eigenfunction) $|\\phi_m \\rangle$. If $\\mathbf{\\hat{O}}$ is to correspond to an observable in the real world, it better be hermitian and unitary, which means the eigenstates $|\\phi_m \\rangle$ form an orthonormal bases:\n",
    "\n",
    "$$ \\langle \\phi_m | \\phi_n \\rangle = \\delta_{mn} $$\n",
    "\n",
    "where $ \\delta_{mn} $ is the Kronecker delta.\n",
    "\n",
    "Say we are in the state $|\\psi \\rangle = \\sum_m a_m | \\phi_m \\rangle$ for some coefficients $a_m$. First, a bit of linear algebra knowledge tells us we can write any state imaginable as a linear combination of eigenstates because the eigenstates form an orthonormal basis.\n",
    "\n",
    "Next, we can recognize that, since we want our state $|\\psi \\rangle$ to be normalized (wavefunctions are always normalized!), and since each of the eigenstates $|\\psi \\rangle$ are normalized that places a constraint on the coeffients $a_m$:\n",
    "\n",
    "$$ \\langle \\psi | \\psi \\rangle = 1 $$\n",
    "\n",
    "$$ \\langle \\psi | \\psi \\rangle = \\left( \\sum_m a_m^* \\langle \\phi_m | \\right)\\left( \\sum_n a_n | \\phi_n \\rangle \\right) $$\n",
    "\n",
    "By orthonormality the cross terms cancel out,\n",
    "\n",
    "$$ \\langle \\psi | \\psi \\rangle = \\sum_m a_m^* a_m \\langle \\phi_m | \\phi_m \\rangle $$\n",
    "\n",
    "So, for our general state $|\\psi \\rangle$ to be normalized $a_m$ may be anything (including being complex) as long as the coeffients obey:\n",
    "\n",
    "$$ 1 = \\sum_m |a_m|^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we dealt with stationary states (eigenfunctions of the hamiltonian), the time evolution of our system simply changed the overall phase of our state, but the time evolution of a superposition is more complicated.\n",
    "\n",
    "For each energy eigenfunctions $\\phi_n$ with corresponding eigenvalue $E_n$, we know the time evolution is given by $e^{-i\\frac{E_n}{\\hbar}t}$. When dealing with a linear combination of eigenfunctions, we can simply include the corresponding temporal component to each term in the linear combination to produce a valid solution of the time dependent Schrödinger equation.\n",
    "\n",
    "$$ | \\psi \\rangle = \\sum_n a_n | \\phi_n \\rangle \\rightarrow |\\Psi(x,t)\\rangle = \\sum_n a_n e^{-i\\frac{E_n}{\\hbar}t} | \\phi_n \\rangle $$\n",
    "\n",
    "where $|\\phi_n \\rangle$ and $E_n$ are the eigenfunction/eigenvalue pairs of the hamiltonian.\n",
    "\n",
    "Now we can observe the time evolution of superposition states, once again using `matplotlib` animations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "np.set_printoptions(precision=4, linewidth=110) # just to make printing arrays slightly nicer\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve our favorite equation with the potential of our choice: for example, a harmonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "L = 100\n",
    "\n",
    "def V(x): # Return the potential energy given the position of each grid element\n",
    "    return 0.001 * x**2\n",
    "    #return 0 * x # In this case, we are setting the potential energy to zero.\n",
    "\n",
    "eta = 1\n",
    "m = 1\n",
    "q = 1\n",
    "\n",
    "X = np.linspace(0,L,num=N) - L/2\n",
    "a = X[1] - X[0] # grid spacing\n",
    "\n",
    "t = -eta**2 / (2 * m * a**2)\n",
    "eps = -2*t + q * V(X)\n",
    "\n",
    "# Define discretized hamiltonian\n",
    "H = t*np.eye(N, k=-1) + eps*np.eye(N) + t*np.eye(N, k=1)\n",
    "\n",
    "# Solve TISE to get wavefunctions and energies\n",
    "vals, vecs = np.linalg.eig(H)\n",
    "order = np.argsort(vals)\n",
    "vals, vecs = vals[order], vecs[:, order]\n",
    "vecs = vecs.T\n",
    "# Normalize wavefunction\n",
    "vecs /= np.sqrt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "plt.plot(X, V(X))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('V(x)')\n",
    "plt.title('Potential Energy Surface')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at arbitrary superpositions we have to reflect on how we solved the Schrödinger equation in the first place. While we discretize space, we can't trust the computed high energy eigenstates/eigenvalues. Consequently, we should choose some cutoff (say 20), and only consider superpositions of states below that cutoff.\n",
    "\n",
    "After that you can specify what ever (unnormalized) superposition of levels you want in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 20 # how many energy levels to trust from our computed results (probably shouldn't be more than 20 or so)\n",
    "psi = vecs[:cutoff].astype(np.complex64)\n",
    "E = vals[:cutoff]\n",
    "coeff = np.zeros(cutoff, dtype=np.complex64)\n",
    "\n",
    "# Here you can set the coefficients of the state to anything you want (don't worry about normalization yet)\n",
    "\n",
    "coeff[4] = 1\n",
    "coeff[0] = 1j\n",
    "coeff[3] = -2.5\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Never forget to normalize the wavefunction - as seen below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    norm = (np.abs(coeff)**2).sum()\n",
    "    assert norm > 0\n",
    "    coeff /= np.sqrt(norm)\n",
    "except AssertionError:\n",
    "    print('Error: Norm is zero, you must set the coeffients to nonzero')\n",
    "    \n",
    "print('\\nCoeffients:', coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebook, we define an `evolve` function for the matplotlib animation to compute the wavefunction at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(coeff, psi, E, timestep=5, num_step=300):\n",
    "    assert len(coeff) == len(psi) == len(E)\n",
    "    ts = np.arange(num_step)*timestep\n",
    "    t = 0\n",
    "    cnt = 0\n",
    "    while num_step is None or cnt < num_step:\n",
    "        c = coeff * np.exp(- 1j * E / eta * t)\n",
    "        cnt += 1\n",
    "        t += timestep\n",
    "        yield t, c @ psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 1\n",
    "num_step = 300\n",
    "\n",
    "if timestep is None:\n",
    "    av_E = (coeff)**2 @ E\n",
    "    timestep = 2*np.pi * eta / av_E / num_step\n",
    "    print('Using timestep: {:.4f}'.format(timestep))\n",
    "    \n",
    "evolution = evolve(coeff, psi, E, timestep=timestep, num_step=num_step)\n",
    "\n",
    "print('\\nReal component in blue, imaginary in red')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "ax1.set_title('Wavefunction')\n",
    "ax1.set_ylabel('psi(x)')\n",
    "ax2.set_title('Probability Density')\n",
    "ax2.set_ylabel('p(x)')\n",
    "ax2.set_xlabel('x')\n",
    "wave = (coeff @ psi)\n",
    "wave_real, = ax1.plot(X, wave.real, c='b')\n",
    "wave_imag, = ax1.plot(X, wave.imag, c='r')\n",
    "p = np.abs(wave)**2\n",
    "dens, = ax2.plot(X, p, color='k')\n",
    "#ax2.fill_between(X, p, color='k', alpha=0.2)\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "lim = np.sqrt(p).max()+0.1\n",
    "ax1.set_ylim(-lim, lim)\n",
    "ax2.set_ylim(-1e-4, max(p)+0.02)\n",
    "\n",
    "def run(data):\n",
    "    # update the data\n",
    "    t, wave = data\n",
    "    \n",
    "    ax1.set_title('Wavefunction (t={:.2f})'.format(t))\n",
    "    \n",
    "    wave_real.set_data(X, wave.real)\n",
    "    wave_imag.set_data(X, wave.imag)\n",
    "\n",
    "    p = np.abs(wave)**2\n",
    "    dens.set_data(X, p)\n",
    "    \n",
    "    #fig.canvas.draw()\n",
    "    \n",
    "    return wave_real, wave_imag, dens\n",
    "\n",
    "ani = animation.FuncAnimation(fig, run, evolution, blit=False, interval=10, repeat=False, save_count=1000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Oscillating Superpositions\n",
    "\n",
    "Test a few different superpositions and observe how the wavefunction and, more importantly, the probability distribution over positions changes with time, then answer the questions below: (A short explanation/example for each question is fine)\n",
    "\n",
    "**Part 1.** How does the behavior compare between:\n",
    "\n",
    "A. Superpositions of only even states vs superpositions of only odd solutions? In responding to the question focus on the overall shape of the wave function and the probability density. There is an important difference in the probability density near x=0.\n",
    "\n",
    "B. Compare the probability density for n=2 and n=100? -- What are the main difference(s)? -- When you made a measurement at different locations along the box, how do the answers for probability differ qualitatively? Assume that a measurement can be made only over a length scale that is equal to one-tenth of the box (this is because the measurement equipment can measure only over a finite length scale).\n",
    "\n",
    "C. Compare the probability density as a function of time for (a) a state with n=10 and (b) a balanced superposition (equal superposition of all states). What is the reason for the difference?\n",
    "\n",
    "**Part 2.** (Ignore this problem) Give an example of a superposition that causes a large amount of probability density to oscillate back and forth uniformly (as one mass). In other words, what superposition produces a \"pseudo-classical\" oscillation in our harmonic well with as large of an amplitude you can achieve. (You don't have to find the optimal superposition, just one that looks good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answers here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement\n",
    "\n",
    "Since we can write any general state in terms of the eigenstates of a particular operator. Thinking about measurement becomes a little tricky with superpositions. The Copenhagen interpretation of quantum mechanics (which is the most popular interpretation) posits that we can't directly measure a superposition of eigenfunctions. Instead, when we make a measurment of a particle in a superposition of eigenfunctions, the wavefunction \"collapses\" irreversibly to one of the eigenfunctions of our operator, and then the outcome we actually observe is the corresponding eigenvalue. Furthermore, the probability that our state $|\\psi \\rangle$ collapses to state $|\\phi_m \\rangle$ is given by $|a_m|^2$.\n",
    "\n",
    "So, in short the probability of measuring eigenvalue $\\alpha_m$ is given by:\n",
    "\n",
    "$$ \\mathrm{Prob}(\\alpha_m) = | \\langle \\phi_m | \\psi \\rangle |^2 $$\n",
    "\n",
    "This interpretation has physical and philosophical implications, which have freaked many of the greatest minds in the past and present. For example, Einstein who received the Nobel prize for his work in quantum mechanics opposed that any aspect of the universe could be nondeterministic. Nevertheless, the Copenhagen interpretation would suggest the outcome of an experiment can in fact be nondeterministic. Crazy, huh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, in quantum mechanics, we have to be a little more careful about what we mean by the position, momentum, or any other observable of our particle. Since measuring an arbitrary state $|\\psi \\rangle$ could result in one of infinitely many outcomes, one common way to think about measurment outcomes is average values (expectation values). For state $|\\psi \\rangle$, the expectation value of some operator $\\mathbf{\\hat{O}}$ is given by:\n",
    "\n",
    "$$ \\langle \\mathbf{\\hat{O}} \\rangle = \\langle \\psi | \\mathbf{\\hat{O}} | \\psi \\rangle = \\sum_m |a_m|^2 \\alpha_m $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Expectation Values\n",
    "\n",
    "Given a particle in state $|\\chi \\rangle$, trapped in a harmonic potential ($V(x) = 0.001 x^2$) with parameters ($\\eta = 1$, $m = 1$, $q = 1$, $N = 300$, and $L = 100$, so $a \\approx \\frac{1}{3}$):\n",
    "\n",
    "$$ | \\chi \\rangle = 2 | \\phi_1 \\rangle + 6i | \\phi_2 \\rangle - 3| \\phi_5 \\rangle $$\n",
    "\n",
    "where $\\phi_m$ is the $m$th eigenstate of the hamiltonian ($\\phi_0$ is the ground state).\n",
    "\n",
    "First normalize $| \\chi \\rangle$, then compute the expectation value of the hamiltonian $\\langle \\mathbf{\\hat{H}} \\rangle$. Compare the result you get in theory and by numerically solving the Schrödinger equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Changing basis (only for EE 521)\n",
    "\n",
    "As we saw earlier, the position and hamiltonian operators do not commute. However, their eigenfunctions form a basis over the same space, so we can write any eigenstate of the position operator as a linear combination of energy eigenstates.\n",
    "\n",
    "Unfortunately, since we are numerically approximating the true hamitonian and position operators of our system, the accuracy of high energy eigenfunctions we compute suffer from the numerical approximations, and therefore can't be trusted.\n",
    "\n",
    "Find the linear combination of the 20 lowest energy eigenfunctions of the particle in the harmonic potential above ($V(x) = 0.001 x^2$) which best approximates the position eigenstate for the particle being at $x=0$. (Using the parameters defined above: $\\eta = 1$, $m = 1$, $q = 1$, $N = 300$, and $L = 100$)\n",
    "\n",
    "Hint: Position eigenstates correspond to Dirac delta functions, eg. the eigenstate for a particle at $x=x_0$ is $\\phi(x) = \\delta(x-x_0)$, so what would a position eigenstate look like in our discretized system.\n",
    "\n",
    "1. After finding the coefficients, plot the superposition at t = 0. \n",
    "\n",
    "2. Plot the superposition as a function of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EE 421 Qiskit",
   "language": "python",
   "name": "ee421"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
